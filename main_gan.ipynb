{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically upload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ray\n",
    "from ray import tune\n",
    "import torch\n",
    "\n",
    "from genome_embeddings import corrupt\n",
    "from genome_embeddings import data_viz\n",
    "from genome_embeddings import models\n",
    "from genome_embeddings import pre_process\n",
    "from genome_embeddings import trainable # import before ray (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_04-11-2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bacterial genomes in dataset: 2718\n",
      "Total number of KOs in dataset: 9874\n"
     ]
    }
   ],
   "source": [
    "tla_to_tnum, keepers = pre_process.genomes2include()\n",
    "org_to_kos, n_kos_tot, all_kos = pre_process.load_kos(tla_to_tnum)\n",
    "org_to_mod_to_kos, mod_sets = pre_process.load_mods()\n",
    "\n",
    "all_kos = torch.load(\"/Users/natasha/Desktop/all_kos_2020-09-29.pt\")\n",
    "org_to_mod_to_kos = torch.load(\"/Users/natasha/Desktop/org_to_mod_to_kos_2020-09-29.pt\")\n",
    "train_data = torch.load(\"/Users/natasha/Desktop/kegg_v2_train_2020-09-29.pt\")\n",
    "test_data = torch.load(\"/Users/natasha/Desktop/kegg_v2_test_2020-09-29.pt\")\n",
    "train_genomes = torch.load(\"/Users/natasha/Desktop/kegg_v2_train_genomes_2020-09-29.pt\")\n",
    "test_genomes = torch.load(\"/Users/natasha/Desktop/kegg_v2_test_genomes_2020-09-29.pt\")\n",
    "\n",
    "mod_to_ko_clean = pre_process.clean_kos(mod_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any genomes with fewer than 500 KOs \n",
    "# Esp. important to remove genomes with 0 KOs (n=35)\n",
    "\n",
    "good_idx_train = train_data.sum(axis=1) > 500\n",
    "good_idx_test = test_data.sum(axis=1) > 500\n",
    "train_data = train_data[good_idx_train,:]\n",
    "test_data = test_data[good_idx_test,:]\n",
    "\n",
    "# to numpy for indexing, then back to list for using\n",
    "train_genomes = list(np.array(train_genomes)[good_idx_train])\n",
    "test_genomes = list(np.array(test_genomes)[good_idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_load = \"2020-10-16_10mods\"\n",
    "\n",
    "# corrupted_train = torch.load(\"/Users/natasha/Desktop/corrupted_train_\"+date_to_load+\".pt\")\n",
    "# c_train_genomes = torch.load(\"/Users/natasha/Desktop/c_train_genomes_\"+date_to_load+\".pt\")\n",
    "# corrupted_test = torch.load(\"/Users/natasha/Desktop/corrupted_test_\"+date_to_load+\".pt\")\n",
    "# c_test_genomes = torch.load(\"/Users/natasha/Desktop/c_test_genomes_\"+date_to_load+\".pt\")\n",
    "\n",
    "corrupted_train = torch.load(DATA_FP+\"corrupted_train_\"+date_to_load+\".pt\")\n",
    "c_train_genomes = torch.load(DATA_FP+\"c_train_genomes_\"+date_to_load+\".pt\")\n",
    "corrupted_test = torch.load(DATA_FP+\"corrupted_test_\"+date_to_load+\".pt\")\n",
    "c_test_genomes = torch.load(DATA_FP+\"c_test_genomes_\"+date_to_load+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #\"cuda\" if use_cuda else \"cpu\")\n",
    "num_features = int(train_data.shape[1]/2)\n",
    "z_dim = num_features * 6\n",
    "\n",
    "# get model instances, set to train mode\n",
    "gen = models.Generator().to('cpu')\n",
    "crit = models.Critic().to('cpu')\n",
    "gen.train()\n",
    "crit.train()\n",
    "\n",
    "# define optimizers\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "lr = 0.001\n",
    "weight_decay = 0.001\n",
    "gen_opt = torch.optim.AdamW(gen.parameters(), lr=lr, betas=(beta_1, beta_2), weight_decay=weight_decay)\n",
    "crit_opt = torch.optim.AdamW(crit.parameters(), lr=lr, betas=(beta_1, beta_2), weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "kfolds = 10\n",
    "loaders = trainable.cv_dataloader_SINGLE(batch_size, num_features, kfolds, corrupted_train, corrupted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9874"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = int(corrupted_train.shape[1]/2)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of genome_embeddings.trainable failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/natasha/Desktop/mcgill_postdoc/ncbi_genomes/genome_embeddings/trainable.py\", line 44, in <module>\n",
      "    class MemCache:\n",
      "  File \"/Users/natasha/Desktop/mcgill_postdoc/ncbi_genomes/genome_embeddings/trainable.py\", line 51, in MemCache\n",
      "    num_features = int(train_data.shape[1]/2)\n",
      "NameError: name 'train_data' is not defined\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 1\n",
    "# crit_repeats = 1 # num times to update critic per generator update\n",
    "# gen_repeats = 1 # num times to update generator per critic update\n",
    "# c_lambda = 10 # the weight of the gradient penalty \n",
    "# critic_losses = []\n",
    "# generator_losses = []\n",
    "# z_dim = n_features * 6\n",
    "# steps = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch_idx, (_, real) in enumerate(loaders[\"train\"]):\n",
    "#         cur_batch_size = len(real)\n",
    "#         real = real.to(device)\n",
    "\n",
    "#         mean_iteration_critic_loss = 0\n",
    "#         for _ in range(crit_repeats):\n",
    "#             ### Update critic ###\n",
    "#             crit_opt.zero_grad()\n",
    "#             fake_noise = trainable.get_noise(cur_batch_size, z_dim, device=device)\n",
    "#             fake = gen(fake_noise)\n",
    "#             crit_fake_pred = crit(fake.detach())\n",
    "#             crit_real_pred = crit(real)\n",
    "            \n",
    "#             # Calculate loss for this iteration\n",
    "#             epsilon = torch.rand(len(real), 1, device=device, requires_grad=True)\n",
    "#             gradient = trainable.get_gradient(crit, real, fake.detach(), epsilon)\n",
    "#             gp = trainable.gradient_penalty(gradient)\n",
    "#             crit_loss = trainable.get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
    "#             mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
    "\n",
    "#             # Update gradients\n",
    "#             crit_loss.backward(retain_graph=True)\n",
    "#             # Update optimizer\n",
    "#             crit_opt.step()\n",
    "            \n",
    "#         critic_losses += [mean_iteration_critic_loss]\n",
    "        \n",
    "#         mean_iteration_gen_loss = 0\n",
    "#         for _ in range(gen_repeats):\n",
    "#             ### Update generator ###\n",
    "#             gen_opt.zero_grad()\n",
    "#             fake_noise_2 = trainable.get_noise(cur_batch_size, z_dim, device=device)\n",
    "#             fake_2 = gen(fake_noise_2)\n",
    "#             crit_fake_pred = crit(fake_2)\n",
    "\n",
    "#             gen_loss = trainable.get_gen_loss(crit_fake_pred)\n",
    "#             gen_loss.backward()\n",
    "#             mean_iteration_gen_loss += gen_loss.item() / gen_repeats\n",
    "#             # Update the weights\n",
    "#             gen_opt.step()\n",
    "\n",
    "#         # Keep track of the average generator loss\n",
    "#         generator_losses += [mean_iteration_gen_loss]\n",
    "        \n",
    "#         steps += 1\n",
    "\n",
    "#         if batch_idx % 1 == 0:\n",
    "#             print(\"epoch\",epoch,\"batch\",batch_idx,\"gen_loss\",gen_loss.item(), \n",
    "#                      \"mean_iteration_critic_loss\",mean_iteration_critic_loss)\n",
    "            \n",
    "#     # if memory usage is high, may be able to free up space by calling garbage collect\n",
    "#     auto_garbage_collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_viz.gan_learning_curve(generator_losses, critic_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = 2000 * 1024 * 1024\n",
    "object_store_memory = 200 * 1024 * 1024\n",
    "driver_object_store_memory=100 * 1024 * 1024\n",
    "ray.shutdown()\n",
    "ray.init(local_mode=True, memory=memory, \n",
    "        object_store_memory=object_store_memory,\n",
    "        driver_object_store_memory=driver_object_store_memory,\n",
    "        num_cpus=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"num_epochs\": 10,\n",
    "         \"kfolds\": 10,\n",
    "         \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "          \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "          \"beta1\": tune.loguniform(0.4, 0.6),\n",
    "          \"beta2\": tune.uniform(0.9, 1),\n",
    "          \"weight_decay\": tune.loguniform(1e-5, 1e-2),\n",
    "          \"architecture\": tune.choice([1,2,3]),\n",
    "          \"crit_to_gen_repeats\": tune.choice([0,1]),\n",
    "          \"c_lambda\": tune.uniform([0,50]),\n",
    "          \"batch_size\": 128,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    trainable.train_GAN, \n",
    "    name=\"nov4_gan\",\n",
    "    config=config,\n",
    "    verbose=2, \n",
    "    resources_per_trial={\n",
    "            \"cpu\": 10,\n",
    "            \"gpu\": 0\n",
    "    },\n",
    "    num_samples=20,  \n",
    "    queue_trials=True,\n",
    "    #local_dir=\"/Users/natasha/Desktop/TUNE_RESULT_DIR\",\n",
    "    local_dir=\"/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_04-11-2020/TUNE_RESULT_DIR\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
