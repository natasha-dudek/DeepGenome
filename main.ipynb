{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically upload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ray\n",
    "from ray import tune\n",
    "import sklearn as sk\n",
    "import torch\n",
    "\n",
    "from genome_embeddings import corrupt\n",
    "from genome_embeddings import data_viz\n",
    "from genome_embeddings import evaluate\n",
    "from genome_embeddings import models\n",
    "from genome_embeddings import pre_process\n",
    "from genome_embeddings import trainable # import before ray (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"CC\" # \"CC\" | \"Desktop\"\n",
    "# must change local_dir=\"/Users/natasha/Desktop/TUNE_RESULT_DIR\"\n",
    "local_dir=\"/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_04-12-2020/TUNE_RESULT_DIR\"\n",
    "\n",
    "#BASE_DIR = '/Users/natasha/Desktop/vae/'\n",
    "BASE_DIR =\"/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_04-12-2020/TUNE_RESULT_DIR\"\n",
    "\n",
    "if mode == \"Desktop\":\n",
    "    settings = Namespace(\n",
    "        DATA_FP = '/Users/natasha/Desktop/mcgill_postdoc/ncbi_genomes/genome_embeddings/data/', \n",
    "        SAVE_FP = '/Users/natasha/Desktop/',\n",
    "        num_epochs = 2,\n",
    "        num_cpus=5, \n",
    "        replacement_threshold = 0.5, # probability over which binarizer converts to a 1\n",
    "        num_corruptions = 100, # number of corrupted versions of a genome to produce\n",
    "    )\n",
    "elif mode == \"CC\":\n",
    "    settings = Namespace(\n",
    "        DATA_FP = '/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_04-12-2020/',\n",
    "        SAVE_FP = '/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_04-12-2020/',\n",
    "        num_epochs = 2,\n",
    "        num_cpus=5, \n",
    "        replacement_threshold = 0.5, # probability over which binarizer converts to a 1\n",
    "        num_corruptions = 100, # number of corrupted versions of a genome to produce\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data needed to make corrupted train + test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bacterial genomes in dataset: 2718\n",
      "Total number of KOs in dataset: 9874\n"
     ]
    }
   ],
   "source": [
    "create_new = False\n",
    "from_scratch = False # controls where in creation process to jump in\n",
    "\n",
    "# Create list of which genomes from KEGG to include in dataset\n",
    "tla_to_tnum, keepers = pre_process.genomes2include()\n",
    "tnum_to_tla = {v:k for k,v in tla_to_tnum.items()}\n",
    "org_to_kos, n_kos_tot, all_kos = pre_process.load_kos(tla_to_tnum)\n",
    "org_to_mod_to_kos, mod_sets = pre_process.load_mods()\n",
    "mod_to_kos = pre_process.create_mod_to_kos(org_to_mod_to_kos)\n",
    "\n",
    "if create_new:\n",
    "    if from_scratch:\n",
    "        # Load list mods and KOs per genome\n",
    "        data, genome_order = pre_process.make_tensor(org_to_mod_to_kos, org_to_kos, n_kos_tot, tla_to_tnum, all_kos)\n",
    "        torch.save(data, BASE_DIR+\"kegg_v2.pt\")\n",
    "        torch.save(genome_order, BASE_DIR+\"genome_order.pt\")\n",
    "    else:\n",
    "        # load data -- tensor (genomes + which KOs are encoded) + genome_order in tensor (tla)\n",
    "        data = torch.load(BASE_DIR+\"kegg_v2.pt\")\n",
    "        genome_order = torch.load(BASE_DIR+\"genome_order.pt\")\n",
    "    \n",
    "    # Create test-train split\n",
    "    train_genomes, test_genomes = pre_process.train_test_split(keepers) # list of IDs to keep\n",
    "    train_data = pre_process.prep_data(train_genomes, all_kos, org_to_kos, \"train\")\n",
    "    test_data = pre_process.prep_data(test_genomes, all_kos, org_to_kos, \"test\")\n",
    "    \n",
    "    torch.save(all_kos, BASE_DIR+\"all_kos_2020-09-29.pt\")\n",
    "    torch.save(org_to_mod_to_kos, BASE_DIR+\"org_to_mod_to_kos_2020-09-29.pt\")\n",
    "    torch.save(train_data, BASE_DIR+\"kegg_v2_train_2020-09-29.pt\")\n",
    "    torch.save(test_data, BASE_DIR+\"kegg_v2_test_2020-09-29.pt\")\n",
    "    torch.save(train_genomes, BASE_DIR+\"kegg_v2_train_genomes_2020-09-29.pt\")\n",
    "    torch.save(test_genomes, BASE_DIR+\"kegg_v2_test_genomes_2020-09-29.pt\")\n",
    "else:\n",
    "    all_kos = torch.load(BASE_DIR+\"all_kos_2020-09-29.pt\")\n",
    "    org_to_mod_to_kos = torch.load(BASE_DIR+\"org_to_mod_to_kos_2020-09-29.pt\")\n",
    "    train_data = torch.load(BASE_DIR+\"kegg_v2_train_2020-09-29.pt\")\n",
    "    test_data = torch.load(BASE_DIR+\"kegg_v2_test_2020-09-29.pt\")\n",
    "    train_genomes = torch.load(BASE_DIR+\"kegg_v2_train_genomes_2020-09-29.pt\")\n",
    "    test_genomes = torch.load(BASE_DIR+\"kegg_v2_test_genomes_2020-09-29.pt\")\n",
    "\n",
    "mod_to_ko_clean = pre_process.clean_kos(mod_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any genomes with fewer than n_min KOs \n",
    "n_min = 500\n",
    "good_idx_train = train_data.sum(axis=1) >= n_min\n",
    "good_idx_test = test_data.sum(axis=1) >= n_min\n",
    "train_data = train_data[good_idx_train,:]\n",
    "test_data = test_data[good_idx_test,:]\n",
    "\n",
    "# to numpy for indexing, then back to list for using\n",
    "train_genomes = list(np.array(train_genomes)[good_idx_train])\n",
    "test_genomes = list(np.array(test_genomes)[good_idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of genomes that encode fewer than 10 modules\n",
    "train_data, train_genomes = pre_process.remove_duds(train_data, train_genomes, tnum_to_tla, org_to_mod_to_kos)\n",
    "test_data, test_genomes = pre_process.remove_duds(test_data, test_genomes, tnum_to_tla, org_to_mod_to_kos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any columns that are all zeros in both the train and test sets (TL;DR yes)\n",
    "good_cols = ((train_data.sum(axis=0) != 0) | (test_data.sum(axis=0) != 0))\n",
    "train_data = train_data[:,good_cols]\n",
    "test_data = test_data[:,good_cols]\n",
    "all_kos = np.array(all_kos)[good_cols].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 77)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data.sum(axis=1) == 0).sum(), (train_data.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1595)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_data.sum(axis=1) == 0).sum(), (test_data.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2305, 9863), (279, 9863))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = 'M00001'\n",
    "# a = list(mod_sets[mod].values())\n",
    "# a.sort(reverse=True)\n",
    "# x_labels = [i for i in range(len(mod_sets[mod].values()))]\n",
    "# plt.bar(x_labels, a)\n",
    "# plt.yscale('log')\n",
    "# plt.title(\"Variants of module \"+mod)\n",
    "# plt.xlabel(\"Variant (n=\"+str(len(mod_sets[mod].values()))+\" )\")\n",
    "# plt.ylabel(\"Count across all genomes (n=\"+str(len(org_to_mod_to_kos))+\")\")\n",
    "# print(max(mod_sets[mod].values()), len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of KOs encoded by each genome\n",
    "# Note: this includes genomes excluded from the final dataset!\n",
    "plt.hist([len(org_to_kos[i]) for i in org_to_kos], 50)\n",
    "plt.xlabel(\"Number of KOs per genome\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(BASE_DIR+\"fig.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(org_to_kos[i]) for i in org_to_kos if i in train_genomes or i in test_genomes]\n",
    "print(np.median(lens), min(lens), max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Plot the number of modules encoded by each genome\n",
    "# n_genomes = len(org_to_mod_to_kos)\n",
    "# temp = [list(org_to_mod_to_kos[i].keys()) for i in org_to_mod_to_kos]\n",
    "# n_mods = len(list(set([item for sublist in temp for item in sublist])))\n",
    "# plt.hist([len(org_to_mod_to_kos[i]) for i in org_to_mod_to_kos], 50)\n",
    "# plt.xlabel(\"Number of modules per genome\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of the # of modules (n=\"+str(n_mods)+\") per genome (n=\"+str(n_genomes)+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [len(org_to_mod_to_kos[org]) for org in org_to_mod_to_kos]\n",
    "# a.sort()\n",
    "# print(\"There are \"+str(a.count(0))+\" genomes with zero modules\")\n",
    "\n",
    "# # Red\n",
    "# # Rue\n",
    "# # pvac\n",
    "# # cgw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT SOME GENOMES DO NOT HAVE A SINGLE MOD AND AREN'T EVEN IN THE DICT (n= 2717-2713 = 5)\n",
    "# E.G.: clap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Count and plot the number of genomes that encode each module\n",
    "# mods_count = defaultdict(int)\n",
    "# for org in org_to_mod_to_kos:\n",
    "#     for mod in org_to_mod_to_kos[org]:\n",
    "#         mods_count[mod] += 1\n",
    "        \n",
    "# plt.hist(mods_count.values(), 50)\n",
    "# plt.xlabel(\"Number of genomes encoding each module\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of the # of genomes (n=\"+str(n_genomes)+\") encoding each module (n=\"+str(n_mods)+\")\")\n",
    "# plt.yscale('log')\n",
    "# print(\"Number of mods encoded in only one genome:\",list(mods_count.values()).count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the percentage of genes per genome that contribute to modules\n",
    "# perc_mod = []\n",
    "# for org in org_to_mod_to_kos:\n",
    "#     try:\n",
    "#         tla = tla_to_tnum[org]\n",
    "#         n_kos = len(org_to_kos[tla])\n",
    "#         n_mod_kos = len([org_to_mod_to_kos[org][mod] for mod in org_to_mod_to_kos[org]])\n",
    "#         perc_mod.append(n_mod_kos/n_kos*100)\n",
    "#     except: KeyError\n",
    "\n",
    "# plt.hist(perc_mod, 50)\n",
    "# plt.xlabel(\"Percent of KOs contributing to modules\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of the % of KOs (n_all=\"+str(n_kos_tot)+\") represented by modules (n=\"+str(n_mods)+\") per genome (n=\"+str(n_genomes)+\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Percentage of KOs that are / are not part of a module\n",
    "\n",
    "# kos_in_mods = []\n",
    "\n",
    "# for org in org_to_mod_to_kos:\n",
    "#     for mod in org_to_mod_to_kos[org]:\n",
    "#         kos_in_mods.extend(org_to_mod_to_kos[org][mod])\n",
    "\n",
    "# kos_in_mods = list(set(kos_in_mods))\n",
    "\n",
    "# print(len(kos_in_mods), len(all_kos), len(kos_in_mods)/len(all_kos)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of KOs that are part of 1 module, 2 modules, 3 modules, etc\n",
    "# #all_kos # unique list of all KOs\n",
    "# ko_counter = defaultdict(list)\n",
    "\n",
    "# for ko in all_kos:\n",
    "#     for org in org_to_mod_to_kos:\n",
    "#         for mod in org_to_mod_to_kos[org]:\n",
    "#             if ko in org_to_mod_to_kos[org][mod]:\n",
    "#                 if mod not in ko_counter[ko]:\n",
    "#                     ko_counter[ko].append(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ko_counter2 = {}\n",
    "# for ko in ko_counter:\n",
    "#     ko_counter2[ko] = len(ko_counter[ko])\n",
    "\n",
    "# for ko in all_kos:\n",
    "#     if ko not in ko_counter2:\n",
    "#         ko_counter2[ko] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = defaultdict(int)\n",
    "# for i in ko_counter2.values():\n",
    "#     a[i] += 1\n",
    "# a, sum(a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ko_counter2.values())\n",
    "# plt.yscale('log')\n",
    "# plt.title(\"Number of modules each KO contributes to\")\n",
    "# plt.xlabel(\"Number of modules\")\n",
    "# plt.ylabel(\"Frequency (# of KOs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrupt input genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-10 genomes: 5000 corrupted versions\n",
    "# 10 - 100 genomes: 1000 corrupted versions\n",
    "# 100-1000 genomes: 500 corrupted versions\n",
    "# 1000 - 10000 genomes: 100 corrupted versions\n",
    "\n",
    "# precorruption (pc) -- get dicts to map genome tla back to list of phylum, class, etc\n",
    "corrupted_train, c_train_genomes, train_input_mods = corrupt.corrupt2(train_data, train_genomes, tnum_to_tla, org_to_mod_to_kos, all_kos, mod_to_ko_clean, org_to_kos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train_genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupt version 1 -- extreme corruption\n",
    "# 2020-09-04: select 1-10 mods for input\n",
    "# 2020-10-16_10mods: select 10 mods for input, genomes with fewer than 10 mods are eliminated\n",
    "# 2020-10-16_1mods: select 1 mods for input, genomes with fewer than 10 mods are eliminated \n",
    "\n",
    "new_corrupt = True\n",
    "date_to_save = \"2020-12-04_10mods\" # uses corrupt2\n",
    "#date_to_save = \"2020-12-03_10mods\" # has no genomes with < 1000 KOs\n",
    "#date_to_load = \"2020-12-02_10mods\" # has no genomes with < 500 KOs \n",
    "#date_to_load = \"2020-10-16_10mods\" # has 9874 features, including a few that are all zeros in both train + test\n",
    "num_corruptions = 100\n",
    "tnum_to_tla = {v:k for k,v in tla_to_tnum.items()}\n",
    "\n",
    "if new_corrupt:\n",
    "    #tnum_to_tla = {v:k for k,v in tla_to_tnum.items()}\n",
    "    corrupted_train, c_train_genomes, train_input_mods = corrupt.corrupt2(train_data, train_genomes, num_corruptions, tnum_to_tla, org_to_mod_to_kos, all_kos, mod_to_ko_clean, org_to_kos)\n",
    "    corrupted_test, c_test_genomes, test_input_mods = corrupt.corrupt2(test_data, test_genomes, num_corruptions, tnum_to_tla, org_to_mod_to_kos, all_kos, mod_to_ko_clean, org_to_kos)\n",
    "    torch.save(corrupted_train, BASE_DIR+\"corrupted_train_\"+date_to_save+\".pt\")\n",
    "    torch.save(c_train_genomes, BASE_DIR+\"c_train_genomes_\"+date_to_save+\".pt\")\n",
    "    torch.save(corrupted_test, BASE_DIR+\"corrupted_test_\"+date_to_save+\".pt\")\n",
    "    torch.save(c_test_genomes, BASE_DIR+\"c_test_genomes_\"+date_to_save+\".pt\")\n",
    "    torch.save(train_input_mods, BASE_DIR+\"train_input_mods_\"+date_to_save+\".pt\")\n",
    "    torch.save(test_input_mods, BASE_DIR+\"test_input_mods_\"+date_to_save+\".pt\")\n",
    "else:\n",
    "    corrupted_train = torch.load(BASE_DIR+\"corrupted_train_\"+date_to_load+\".pt\")\n",
    "    c_train_genomes = torch.load(BASE_DIR+\"c_train_genomes_\"+date_to_load+\".pt\")\n",
    "    corrupted_test = torch.load(BASE_DIR+\"corrupted_test_\"+date_to_load+\".pt\")\n",
    "    c_test_genomes = torch.load(BASE_DIR+\"c_test_genomes_\"+date_to_load+\".pt\")\n",
    "    train_input_mods = torch.load(BASE_DIR+\"train_input_mods_\"+date_to_load+\".pt\")\n",
    "    test_input_mods = torch.load(BASE_DIR+\"test_input_mods_\"+date_to_load+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that there are no all-zero rows\n",
    "non_zero_idx = corrupted_train.sum(axis=1) > n_min\n",
    "len(non_zero_idx), corrupted_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train.shape, corrupted_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, labels, y3 = data_viz.plot_tax_dist(c_train_genomes, c_test_genomes)\n",
    "plt.savefig(BASE_DIR+\"fig_dist.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(c_genomes, org_to_kos):\n",
    "    lens = []\n",
    "    for i in list(set(c_genomes)):\n",
    "        tnum = tla_to_tnum[i]\n",
    "        lens.append(len(org_to_kos[tnum]))\n",
    "    return lens\n",
    "\n",
    "train_lens = get_lengths(c_train_genomes, org_to_kos)\n",
    "test_lens = get_lengths(c_test_genomes, org_to_kos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of KOs encoded by each genome\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(train_lens, 50, color='#3385ff')\n",
    "plt.hist(test_lens, 50, color='#bb99ff')\n",
    "ax.legend(['Train', 'Test'])\n",
    "plt.xlabel(\"# annotated genes / genome\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(BASE_DIR+\"fig.png\")\n",
    "plt.ylim(0,125)\n",
    "plt.xlim(0,max(train_lens)+10)\n",
    "plt.savefig(BASE_DIR+\"fig_ko_dist.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many modules are there per training genome?\n",
    "from collections import defaultdict\n",
    "mods_count = defaultdict(int)\n",
    "for org in org_to_mod_to_kos:\n",
    "    if org in c_train_genomes:\n",
    "        mods_count[org] = len(org_to_mod_to_kos[org])\n",
    "plt.hist(mods_count.values())\n",
    "plt.xlabel('Number of modules')\n",
    "plt.ylabel('Number of genomes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO w/ Ray Tune: Define and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = 2000 * 1024 * 1024\n",
    "object_store_memory = 200 * 1024 * 1024\n",
    "driver_object_store_memory=100 * 1024 * 1024\n",
    "ray.shutdown()\n",
    "ray.init(local_mode=True, memory=memory, \n",
    "        object_store_memory=object_store_memory,\n",
    "        driver_object_store_memory=driver_object_store_memory,\n",
    "        num_cpus=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"num_epochs\": 10,\n",
    "         \"kfolds\": 10,\n",
    "         \"replacement_threshold\": settings.replacement_threshold,\n",
    "         \"nn_layers\": tune.choice([1, 2, 3, 4]),\n",
    "         \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "          \"lr\": tune.loguniform(1e-4, 1e-1), \n",
    "          \"weight_decay\": tune.loguniform(1e-5, 1e-2) \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    trainable.train_AE, \n",
    "    name=\"04_12_2020_vae\",\n",
    "    config=config,\n",
    "    verbose=2, \n",
    "    resources_per_trial={\n",
    "            \"cpu\": 10,\n",
    "            \"gpu\": 0\n",
    "    },\n",
    "    num_samples=20,  \n",
    "    queue_trials=True,\n",
    "    #local_dir=\"/Users/natasha/Desktop/TUNE_RESULT_DIR\",\n",
    "    local_dir=\"/home/ndudek/projects/def-dprecup/ndudek/hp_tuning_01-10-2020/TUNE_RESULT_DIR\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best config is:\", analysis.get_best_config(metric=\"test_f1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.get_best_config(metric=\"test_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single model: Define and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_layers = 3\n",
    "weight_decay = 0.1\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "kfolds = 10\n",
    "num_epochs = 10\n",
    "replacement_threshold = 0.5\n",
    "\n",
    "kld0, bce0, train_losses, test_losses, train_f1s, test_f1s, model = trainable.train_single_vae(nn_layers, weight_decay, lr, batch_size, kfolds, num_epochs, replacement_threshold, corrupted_train, corrupted_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_DIR+\"model.pt\")\n",
    "torch.save(train_losses, BASE_DIR+\"train_losses.pt\")\n",
    "torch.save(test_losses, BASE_DIR+\"test_losses.pt\")\n",
    "torch.save(bce0, BASE_DIR+\"bce0.pt\")\n",
    "torch.save(kld0, BASE_DIR+\"kld0.pt\")\n",
    "torch.save(train_f1s, BASE_DIR+\"train_f1s.pt\")\n",
    "torch.save(test_f1s, BASE_DIR+\"test_f1s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "if load_model:\n",
    "    from genome_embeddings import models\n",
    "    n_features = int(corrupted_train.shape[1]/2)\n",
    "    model = models.VariationalAutoEncoder(n_features, 3)\n",
    "    model.load_state_dict(torch.load(BASE_DIR+\"model.pt\"))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-10 modules per input genome: 0.74\n",
    "# 1 module per input genome: 0.64\n",
    "# 10 modules per input genome: 0.81\n",
    "n_features = int(corrupted_test.shape[1]/2)\n",
    "corrupted = corrupted_test[:,:n_features]\n",
    "uncorrupted = corrupted_test[:,n_features:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model.forward(corrupted)[0].detach()\n",
    "binary_pred = evaluate.eval_binarize(pred, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate test set F1 score (micro-average) and calculate TNs, FPs, FNs, TPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sk.metrics.f1_score(uncorrupted, binary_pred, zero_division=0, average='micro')\n",
    "print(\"Test set F1 score\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s, fig = evaluate.test_f1s(uncorrupted, binary_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tns, fps, fns, tps, total = evaluate.glom_confusion(uncorrupted, binary_pred)\n",
    "print(round(sum(tns)/total*100,2), round(sum(fps)/total*100,2), round(sum(fns)/total*100,2), round(sum(tps)/total*100,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine at BCE vs KLD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BCE and KLD vs experience\n",
    "data_viz.kld_vs_bce(kld0,bce0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = data_viz.learning_curve(train_losses, test_losses, train_f1s, test_f1s)\n",
    "plt.savefig(BASE_DIR+\"learning_curves.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curve and generate AUC score (micro-average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = int(corrupted_test.shape[1]/2)\n",
    "true_genomes = corrupted_test[:,num_features:]\n",
    "fig = data_viz.my_roc_curve(true_genomes.numpy(), pred.numpy())\n",
    "fig.savefig(BASE_DIR+\"roc_fig.png\", dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the input genes present in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel y axis by % rather than count\n",
    "fig = evaluate.compare_in_n_out(binary_pred, corrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the best and worst performing instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# babt = Brucella abortus\n",
    "idx_best = f1s.index(max(f1s))\n",
    "tla_best = c_test_genomes[idx_best]\n",
    "idx_best, tla_best, f1s[idx_best], tla_to_tnum[tla_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmv = Nitrospira moscoviensis\n",
    "# seny = Pectobacteriaceae sodalis (Gammaprot), endosymbiont of Henestaris halophilus\n",
    "idx_worst = f1s.index(min(f1s))\n",
    "tla_worst = c_test_genomes[idx_worst]\n",
    "idx_worst, tla_worst, f1s[idx_worst], tla_to_tnum[tla_worst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the best scoring instance, plot pixel diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_best\n",
    "fig = data_viz.genome_heatmap2(corrupted_test, idx, model)\n",
    "fig.savefig(BASE_DIR+\"fig4.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num genes on in uncorrupted idx_best genome\", torch.sum(uncorrupted[idx_best,:]))\n",
    "print(\"Num genes off in uncorrupted idx_best genome\", n_features - torch.sum(uncorrupted[idx_best,:]))\n",
    "print(\"Num genes on in corrupted idx_best genome\", torch.sum(corrupted[idx_best,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the worst scoring instance, plot pixel diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = idx_worst\n",
    "fig = data_viz.genome_heatmap2(corrupted_test, idx, model)\n",
    "fig.savefig(BASE_DIR+\"fig4.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print median TNs, FPs, FNs, TPs\n",
    "np.median(tns), np.median(fps), np.median(fns), np.median(tps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the model perform on inputs originating from different phyla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dicts to map genome tla back to list of phylum, class, etc\n",
    "train_tax_dict, test_tax_dict = data_viz.tax_distribution(c_train_genomes, c_test_genomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phyla, test_phyla = evaluate.f1s_per_phylum(train_tax_dict, test_tax_dict, c_test_genomes, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bars represent median absolute deviation\n",
    "fig = evaluate.plot_f1_per_phylum(test_phyla)\n",
    "fig.savefig(BASE_DIR+\"f1_per_phylum.png\", dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phyla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x = [1, 1, 1]\n",
    "y = [2, 2, 2]\n",
    "z = [2, 2]\n",
    "stats.kruskal(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluate.plot_count_vs_f1s(train_phyla, test_phyla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be an effect of number of training genomes / phylum on test set per phylum F1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many times is each mod used in the various corrupted inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times each mod occurs in the training set corrupted genomes\n",
    "train_out = evaluate.train_out(train_input_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluate.plot_train_count_hist(train_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does genome size correlate with F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis = # genes in input genome\n",
    "# y-axis = F1 score of reconstructed genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genes_uncorrupted = torch.sum(uncorrupted, 1).numpy().tolist() # get sum of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(n_genes_uncorrupted, f1s, marker='.', s = 1)\n",
    "plt.xlabel(\"# genes in uncorrupted genome\")\n",
    "plt.ylabel(\"F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genes_corrupted = torch.sum(corrupted, 1).numpy().tolist() # get sum of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(n_genes_corrupted, f1s, marker='.', s = 1)\n",
    "plt.xlabel(\"# genes in corrupted input\")\n",
    "plt.ylabel(\"F1 score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much variance is there in the F1 score of genomes encoding certain modules?\n",
    "Do some modules / lifestyles get reconstructed better than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_to_mod, mod_to_proc = evaluate.map_proc_mod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, done = evaluate.plot_mod_count_vs_f1(test_input_mods, c_test_genomes, train_input_mods, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluate.plot_mod_vs_f1(test_input_mods, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot whether number of modules in process correlates with F1 score\n",
    "# More interchangeable options = worse performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess_to_mod, mod_to_subproc = evaluate.map_subproc_mod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = evaluate.plot_metab_pathway_f1(subprocess_to_mod, test_input_mods, f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze F1 scores of actual KOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS MUST BE USED IN MULTIPLE PLACES\n",
    "ko_f1s = []\n",
    "for i in range(uncorrupted.shape[1]): # for every column\n",
    "    f1 = sk.metrics.f1_score(uncorrupted[:,i], binary_pred[:,i], zero_division=0)\n",
    "    ko_f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = 0\n",
    "for i in ko_f1s:\n",
    "    if i == 0:\n",
    "        zeros += 1\n",
    "print(zeros, len(ko_f1s), zeros/len(ko_f1s)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the training set, how many KOs are always zero?\n",
    "zeros_train = train_data.sum(axis=0) > 0\n",
    "n_ones = np.sum(zeros_train)\n",
    "n_zeros_train = len(zeros_train) - n_ones\n",
    "print(\"There are\",n_zeros_train,\"genes that are always off in the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(ko_f1s, bins = 50)\n",
    "plt.xlabel(\"F1 score per gene\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene occurence in uncorrupted ds vs F1 score\n",
    "#ko_counts = torch.sum(uncorrupted, 0)\n",
    "# get gene occurence in uncorrupted training set\n",
    "tr_uncorrupted = corrupted_train[:,n_features:]\n",
    "ko_counts = torch.sum(tr_uncorrupted, 0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(ko_counts, ko_f1s, marker='.', s = 1)\n",
    "ax.set_xlim(0, tr_uncorrupted.shape[0])\n",
    "ax.set_ylim(0,1)\n",
    "plt.xlabel(\"gene count in uncorrupted train set\")\n",
    "plt.ylabel(\"per gene test F1 score\")\n",
    "plt.xticks(rotation=-70)\n",
    "print(\"max KO count:\",int(max(ko_counts)), \", total number of training genomes:\",tr_uncorrupted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on KOs + what metab pathway they are part of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluate.plot_metab_pathway_f1_v2(process_to_mod, mod_to_kos, all_kos, ko_f1s, (7,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# can also do horizontal boxplot with fig = evaluate.plot_metab_pathway_f1_v2\n",
    "fig = evaluate.plot_metab_pathway_f1_v2_horizontal(subprocess_to_mod, mod_to_kos, all_kos, ko_f1s, (5,10))\n",
    "fig.savefig(BASE_DIR+\"f1_per_proc.png\", dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How does the model do on different types of genes (kinases vs transferases, etc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the count of each mod in training set (pre-corruption) correlate with median F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluate.plot_mod_count_vs_f1_v2(test_input_mods, f1s, train_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: randomly turn on n_rand bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = True\n",
    "\n",
    "if new:\n",
    "    print(\"Generating new baseline 1\")\n",
    "    baseline1 = evaluate.baseline1(corrupted_train, org_to_mod_to_kos, org_to_kos, tla_to_tnum, c_train_genomes, corrupted_test)\n",
    "    torch.save(baseline1, BASE_DIR+\"baseline1.pt\")\n",
    "else:\n",
    "    print(\"Loading previously made baseline 1\")\n",
    "    baseline1 = torch.load(BASE_DIR+\"baseline1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sk.metrics.f1_score(uncorrupted, baseline1, zero_division=0, average='micro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tns, fps, fns, tps, total = evaluate.glom_confusion(uncorrupted, baseline1)\n",
    "print(round(sum(tns)/total*100,2), round(sum(fps)/total*100,2), round(sum(fns)/total*100,2), round(sum(tps)/total*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve, generate AUC score (micro-average)\n",
    "num_features = int(corrupted_test.shape[1]/2)\n",
    "fig = data_viz.my_roc_curve(uncorrupted.numpy(), baseline1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: randomly turn on n_rand bits with the highest probability of being on across the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = True\n",
    "\n",
    "if new:\n",
    "    print(\"Generating new baseline 2\")\n",
    "    baseline2 = evaluate.baseline2(corrupted_test, org_to_mod_to_kos, org_to_kos, tla_to_tnum, c_train_genomes, corrupted_test)\n",
    "    torch.save(baseline2, BASE_DIR+\"baseline2.pt\")\n",
    "else:\n",
    "    print(\"Loading previously made baseline 2\")\n",
    "    baseline2 = torch.load(BASE_DIR+\"baseline2.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sk.metrics.f1_score(uncorrupted, baseline2, zero_division=0, average='micro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tns, fps, fns, tps, total = evaluate.glom_confusion(uncorrupted, baseline2)\n",
    "print(round(sum(tns)/total*100,2), round(sum(fps)/total*100,2), round(sum(fns)/total*100,2), round(sum(tps)/total*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve, generate AUC score (micro-average)\n",
    "num_features = int(corrupted_test.shape[1]/2)\n",
    "fig = data_viz.my_roc_curve(uncorrupted.numpy(), baseline2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 3: Create predictions using an untrained version of the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = True\n",
    "\n",
    "if new:\n",
    "    print(\"Generating new baseline 3\")\n",
    "    n_features = int(corrupted_test.shape[1]/2)\n",
    "    fake_model = models.VariationalAutoEncoder(n_features, 3)\n",
    "    fake_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = fake_model.forward(corrupted_test[:,:n_features])[0].detach()\n",
    "    baseline3 = evaluate.eval_binarize(pred, 0.5)\n",
    "    torch.save(baseline3, BASE_DIR+\"baseline3.pt\")\n",
    "else:\n",
    "    print(\"Loading previously made baseline 3\")\n",
    "    baseline3 = torch.load(BASE_DIR+\"baseline3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sk.metrics.f1_score(uncorrupted.long(), baseline3.long(), zero_division=0, average='micro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tns, fps, fns, tps, total = evaluate.glom_confusion(uncorrupted, baseline3)\n",
    "print(round(sum(tns)/total*100,2), round(sum(fps)/total*100,2), round(sum(fns)/total*100,2), round(sum(tps)/total*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve, generate AUC score (micro-average)\n",
    "num_features = int(corrupted_test.shape[1]/2)\n",
    "fig = data_viz.my_roc_curve(uncorrupted.numpy(), baseline3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 4: Always predict the smallest genome in the training set (Hoaglandella endobia -- hed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = True\n",
    "\n",
    "if new:\n",
    "    print(\"Generating new baseline 4\")\n",
    "    baseline4 = evaluate.baseline4(corrupted_train, corrupted_test, tla_to_tnum, org_to_kos, c_train_genomes)\n",
    "    torch.save(baseline4, BASE_DIR+\"baseline4.pt\")\n",
    "else:\n",
    "    print(\"Loading previously made baseline 4\")\n",
    "    baseline4 = torch.load(BASE_DIR+\"baseline4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sk.metrics.f1_score(uncorrupted, baseline4, zero_division=0, average='micro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tns, fps, fns, tps, total = evaluate.glom_confusion(uncorrupted, baseline4)\n",
    "print(round(sum(tns)/total*100,2), round(sum(fps)/total*100,2), round(sum(fns)/total*100,2), round(sum(tps)/total*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve, generate AUC score (micro-average)\n",
    "num_features = int(corrupted_test.shape[1]/2)\n",
    "fig = data_viz.my_roc_curve(uncorrupted.numpy(), baseline4.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 5: Always predict the largest genome in the training set (Paraburkholderia caribensis -- bcai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = True\n",
    "\n",
    "if new:\n",
    "    print(\"Generating new baseline 5\")\n",
    "    baseline5, largest_tla = evaluate.baseline5(corrupted_train, corrupted_test, tla_to_tnum, org_to_kos, c_train_genomes)\n",
    "    torch.save(baseline5, BASE_DIR+\"baseline5.pt\")\n",
    "else:\n",
    "    print(\"Loading previously made baseline 5\")\n",
    "    baseline5 = torch.load(BASE_DIR+\"baseline5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sk.metrics.f1_score(uncorrupted, baseline5, zero_division=0, average='micro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tns, fps, fns, tps, total = evaluate.glom_confusion(uncorrupted, baseline5)\n",
    "print(round(sum(tns)/total*100,2), round(sum(fps)/total*100,2), round(sum(fns)/total*100,2), round(sum(tps)/total*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve, generate AUC score (micro-average)\n",
    "num_features = int(corrupted_test.shape[1]/2)\n",
    "fig = data_viz.my_roc_curve(uncorrupted.numpy(), baseline5.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential model improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase dataset set (more corruptions per genome)\n",
    "# For genomes it does poorly on, make more copies\n",
    "# HP tuning\n",
    "# Remove genomes with <1000 KOs ---> get rid of endosymbionts --- will need to redo small genome baseline\n",
    "# Apply different amounts of KLD importance during training\n",
    "# Make 100% of input genes in output (currently ~20% have 100% in and out). Loss mod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We performed a parameter sweep over warmups (κ) (0.01, 0.05, 0.1, and 1). \n",
    "# κ controls how much the KL divergence loss contributes to learning, \n",
    "# which effectively transitions a deterministic autoencoder to a VAE.28,29 \n",
    "# For instance, a κ = 0.1 would add 0.1 to a weight on the KL loss after each epoch. \n",
    "# After 10 epochs, the KL loss will have equal weight as the reconstruction loss. \n",
    "# We did not observe κ to influence model training (Figure 1B), so we kept κ = 1 for downstream analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model output to KAAS input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = BASE_DIR+'prot_out.txt'\n",
    "ko_new = evaluate.new_genome_random(mod_to_kos, model, all_kos, save_to, BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ko_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
